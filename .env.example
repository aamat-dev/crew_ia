# ==============================
# CONFIGURATION LLM - CREW IA
# ==============================

# === Presets commentés ===
# --- DEV : Ollama only ---
# LLM_DEFAULT_PROVIDER=ollama
# LLM_DEFAULT_MODEL=llama3.1:8b
# SUPERVISOR_PROVIDER=ollama
# SUPERVISOR_MODEL=llama3.1:8b
# MANAGER_PROVIDER=ollama
# MANAGER_MODEL=llama3.1:8b
# EXECUTOR_PROVIDER=ollama
# EXECUTOR_MODEL=llama3.1:8b
# RECRUITER_PROVIDER=ollama
# RECRUITER_MODEL=llama3.1:8b
#
# --- PROD : OpenAI only ---
# LLM_DEFAULT_PROVIDER=openai
# LLM_DEFAULT_MODEL=gpt-4o-mini
# SUPERVISOR_PROVIDER=openai
# SUPERVISOR_MODEL=gpt-4o-mini
# MANAGER_PROVIDER=openai
# MANAGER_MODEL=gpt-4o-mini
# EXECUTOR_PROVIDER=openai
# EXECUTOR_MODEL=gpt-4o-mini
# RECRUITER_PROVIDER=openai
# RECRUITER_MODEL=gpt-4o-mini

# Fournisseur et modèle par défaut (utilisé si aucun spécifique à l'agent)
LLM_DEFAULT_PROVIDER=ollama
LLM_DEFAULT_MODEL=llama3.1:8b

# Ordre de fallback : liste séparée par virgules
LLM_FALLBACK_ORDER=ollama,openai

# Paramètres communs aux LLM
LLM_TIMEOUT_S=60
LLM_DEFAULT_TEMPERATURE=0.2
LLM_DEFAULT_MAX_TOKENS=1500

# ==============================
# CONFIGURATION OLLAMA
# ==============================
OLLAMA_BASE_URL=http://127.0.0.1:11434
OLLAMA_MODEL=llama3.1:8b

# Modèle fallback côté Ollama
OLLAMA_FALLBACK_MODEL=llama3.1:8b

# ==============================
# CONFIGURATION OPENAI
# ==============================
OPENAI_API_KEY=openai_api_key_here
OPENAI_BASE_URL=https://api.openai.com/v1

# --- OpenAI backoff / retries (optionnel) ---
OPENAI_MAX_RETRIES=2
OPENAI_BACKOFF_BASE_MS=200
OPENAI_BACKOFF_FACTOR=2.0

# Modèle fallback côté OpenAI
OPENAI_FALLBACK_MODEL=gpt-4o-mini

# --- Estimation de coût (optionnel) : prix par 1K tokens ---
# Exemple pour gpt-4o-mini (valeurs indicatives !) :
OPENAI_PRICE_PROMPT_GPT_4O_MINI_PER_1K=0.005
OPENAI_PRICE_COMPLETION_GPT_4O_MINI_PER_1K=0.015

# ==============================
# CONFIGURATION STOCKAGE
# ==============================
DB_URL=postgresql://user:password@localhost:5432/crew_ia
RUNS_ROOT=.runs
ARTIFACTS_DIR=.runs

# ==============================
# PARAMÈTRES PIPELINE
# ==============================
MAX_CONCURRENCY=3
NODE_RETRIES=2
RETRY_BASE_DELAY=1.0
ORCH_MAX_CONCURRENCY=4

# --- Overrides par rôle (optionnels) ---
SUPERVISOR_PROVIDER=openai
SUPERVISOR_MODEL=gpt-4o-mini
SUPERVISOR_TIMEOUT_S=40
SUPERVISOR_TEMPERATURE=0.1
SUPERVISOR_MAX_TOKENS=1200

MANAGER_PROVIDER=openai
MANAGER_MODEL=gpt-4o-mini
MANAGER_TIMEOUT_S=90
MANAGER_TEMPERATURE=0.2
MANAGER_MAX_TOKENS=1500

EXECUTOR_TIMEOUT_S=120
EXECUTOR_TEMPERATURE=0.3
EXECUTOR_MAX_TOKENS=2000
EXECUTOR_PROVIDER=ollama
EXECUTOR_MODEL=llama3.1:8b

RECRUITER_PROVIDER=openai
RECRUITER_MODEL=gpt-4o-mini
RECRUITER_TIMEOUT_S=60
RECRUITER_TEMPERATURE=0.2
RECRUITER_MAX_TOKENS=1000

# Logging
LOG_LEVEL=DEBUG       # DEBUG pour plus de détails
LOG_TO_STDOUT=1       # 1 = logs aussi en console, 0 = fichier uniquement

# Active les métriques Prometheus (0 désactivé, 1 activé)
METRICS_ENABLED=0

# DSN Sentry
SENTRY_DSN=

# Environnement Sentry
SENTRY_ENV=dev

# Version de release pour Sentry
RELEASE=crew_ia@0.1.0

# === PostgreSQL & Alembic ===
POSTGRES_USER=crew
POSTGRES_PASSWORD=crew
POSTGRES_DB=crew

# Async driver for app (UNIQUE, et cohérent avec ci-dessus)
DATABASE_URL=postgresql+asyncpg://crew:crew@localhost:5432/crew

# Sync driver for alembic
ALEMBIC_DATABASE_URL=postgresql+psycopg://crew:crew@localhost:5432/crew

# Auth API  (⚠️ comme il y a un # dans la valeur, on MET DES GUILLEMETS)
API_KEY=DaSnC6jaDXoXnnd

# CORS
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:5173
NEXT_PUBLIC_API_URL=http://127.0.0.1:8000

STORAGE_ORDER=file,pg

# Variables Dashboard (Vite)
VITE_API_BASE_URL=http://localhost:8000
VITE_API_TIMEOUT_MS=15000
VITE_DEMO_API_KEY=
